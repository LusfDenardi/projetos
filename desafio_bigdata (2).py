# -*- coding: utf-8 -*-
"""Desafio_Bigdata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l3g2i-DVQWwYhA9dnAksEiO78zApHDcF
"""

# 01) Carregar transações (Polars, lazy) e gerar série semanal total — robusto ao dtype da data

import polars as pl
import matplotlib.pyplot as plt

# Caminho do parquet de transações (2022)
PATH_TX = "/content/drive/MyDrive/part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet"

# Lazy scan
lf_tx = pl.scan_parquet(PATH_TX)

# Detecta colunas de data e quantidade
schema = lf_tx.collect_schema()
names_lc = {n.lower(): n for n in schema.names()}
date_candidates = ["transaction_date", "date", "data", "datetime", "timestamp"]
qty_candidates  = ["quantity", "qty", "quantidade", "qtd", "qtdade"]

date_col = next((names_lc[c] for c in date_candidates if c in names_lc), None)
qty_col  = next((names_lc[c] for c in qty_candidates  if c in names_lc), None)
if date_col is None or qty_col is None:
    raise RuntimeError(f"Colunas não encontradas. Disponíveis: {schema.names()}")

# Normaliza data -> Datetime (sem usar .str em colunas não-string)
dt_dtype = schema[date_col]
if dt_dtype in (pl.Date, pl.Datetime):
    dt_expr = pl.col(date_col).cast(pl.Datetime, strict=False)
elif dt_dtype == pl.Utf8:
    dt_expr = pl.col(date_col).str.strptime(pl.Datetime, strict=False, exact=False)
else:
    raise RuntimeError(f"dtype de {date_col} não suportado: {dt_dtype}. Esperado Date/Datetime ou Utf8.")

# Série semanal total
lf_weekly = (
    lf_tx
    .with_columns(dt_expr.alias("_dt"))
    .with_columns(pl.col("_dt").dt.truncate("1w").alias("week"))
    .group_by("week")
    .agg(pl.col(qty_col).sum().alias("qty_sum"))
    .sort("week")
)

df_weekly = lf_weekly.collect()

# Plot
plt.figure(figsize=(7,4))
plt.plot(df_weekly["week"].to_numpy(), df_weekly["qty_sum"].to_numpy())
plt.title("Vendas semanais (total, 2022)")
plt.xlabel("Semana")
plt.ylabel("Quantidade")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# 02) EDA rápida — histograma com KDE, describe() e boxplot (usando df_weekly da célula anterior)

import polars as pl
import seaborn as sns
import matplotlib.pyplot as plt

# Vetor de quantidades semanais
x = df_weekly["qty_sum"].to_numpy()

# Resumo estatístico (Polars)
print("Resumo estatístico de qty_sum (semanal):")
print(pl.DataFrame({"qty_sum": df_weekly["qty_sum"]}).describe())

# Histograma + KDE
plt.figure(figsize=(6,4))
sns.histplot(x, kde=True)
plt.title("Distribuição de vendas semanais (qty_sum)")
plt.xlabel("Quantidade")
plt.ylabel("Frequência")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Boxplot
plt.figure(figsize=(6,2.8))
sns.boxplot(x=x)
plt.title("Boxplot — vendas semanais (qty_sum)")
plt.xlabel("Quantidade")
plt.grid(True, axis="x", alpha=0.3)
plt.tight_layout()
plt.show()

# 03) Diagnóstico do pico — usa chaves reais (internal_store_id / internal_product_id)

import polars as pl

# Pré-requisitos: lf_tx, df_weekly, date_col, qty_col já definidos nas células anteriores.

# 1) Semana do pico e total
week_max = df_weekly.sort("qty_sum", descending=True)["week"][0]
total_week_qty = float(df_weekly.sort("qty_sum", descending=True)["qty_sum"][0])

# 2) Colunas reais de PDV/SKU
pdv_col = "internal_store_id"
sku_col = "internal_product_id"

# 3) Normaliza data -> Datetime (compatível com Date/Datetime/Utf8)
schema = lf_tx.collect_schema()
dt_dtype = schema[date_col]
if dt_dtype in (pl.Date, pl.Datetime):
    dt_expr = pl.col(date_col).cast(pl.Datetime, strict=False)
elif dt_dtype == pl.Utf8:
    dt_expr = pl.col(date_col).str.strptime(pl.Datetime, strict=False, exact=False)
else:
    raise RuntimeError(f"dtype de {date_col} não suportado: {dt_dtype}")

lf_week = (
    lf_tx
    .with_columns(dt_expr.alias("_dt"))
    .with_columns(pl.col("_dt").dt.truncate("1w").alias("week"))
)

# 4) Top-10 PDVs na semana do pico
df_top_pdv = (
    lf_week
    .filter(pl.col("week") == pl.lit(week_max, dtype=pl.Datetime))
    .group_by(pdv_col)
    .agg(pl.col(qty_col).sum().alias("qty_sum"))
    .sort("qty_sum", descending=True)
    .limit(10)
    .collect()
    .with_columns((pl.col("qty_sum") / total_week_qty * 100).round(2).alias("share_pct"))
)

# 5) Top-10 SKUs na semana do pico
df_top_sku = (
    lf_week
    .filter(pl.col("week") == pl.lit(week_max, dtype=pl.Datetime))
    .group_by(sku_col)
    .agg(pl.col(qty_col).sum().alias("qty_sum"))
    .sort("qty_sum", descending=True)
    .limit(10)
    .collect()
    .with_columns((pl.col("qty_sum") / total_week_qty * 100).round(2).alias("share_pct"))
)

print(f"Semana do pico: {week_max}")
print(f"Total da semana (qty_sum): {int(total_week_qty):,}".replace(",", "."))

print("\nTop-10 PDVs — contribuição na semana do pico:")
print(df_top_pdv)

print("\nTop-10 SKUs — contribuição na semana do pico:")
print(df_top_sku)

# 04) Verificação do pico — janela semanal ao redor + comparação com reference_date + breakdown diário
# Saídas:
#   (A) Janela de 8 semanas ao redor do pico (4 antes, pico, 3 depois) usando transaction_date
#   (B) Mesma janela, mas agregando por reference_date (comparação)
#   (C) Breakdown diário dentro da semana do pico (transaction_date)
#
# Pré-requisitos desta sessão:
#  - lf_tx        : LazyFrame de transações (2022)
#  - date_col     : nome da coluna de data das transações (ex.: "transaction_date")
#  - qty_col      : nome da coluna de quantidade          (ex.: "quantity")
#  - week_max     : datetime (início da semana do pico) obtido anteriormente
#  - chaves reais: internal_store_id / internal_product_id existem no schema
#
# Obs.: Mantém o mesmo bucket de semana via dt.truncate("1w") para ser consistente com as células anteriores.

import polars as pl
from datetime import timedelta
import pandas as pd  # apenas para conversão robusta de tipos de data

# --- parâmetros da janela ---
PRE_WKS  = 4   # semanas antes do pico
POST_WKS = 3   # semanas depois do pico

# --- colunas de valor e chaves (confirmadas no schema informado) ---
NET_COL   = "net_value"
GROSS_COL = "gross_value"
PDV_COL   = "internal_store_id"
SKU_COL   = "internal_product_id"
REF_COL   = "reference_date"

# --- helpers de dtype para data ---
schema = lf_tx.collect_schema()

def _to_dt_expr(col_name: str) -> pl.Expr:
    dt_dtype = schema[col_name]
    if dt_dtype in (pl.Date, pl.Datetime):
        return pl.col(col_name).cast(pl.Datetime, strict=False)
    elif dt_dtype == pl.Utf8:
        return pl.col(col_name).str.strptime(pl.Datetime, strict=False, exact=False)
    else:
        raise RuntimeError(f"dtype de {col_name} não suportado: {dt_dtype}. Esperado Date/Datetime/Utf8.")

dt_expr_tx  = _to_dt_expr(date_col)     # transaction_date -> Datetime
dt_expr_ref = _to_dt_expr(REF_COL)      # reference_date   -> Datetime

# --- define janela temporal (pico ±) ---
wk0  = pd.Timestamp(week_max).to_pydatetime()   # normaliza para datetime nativo
start = wk0 - timedelta(weeks=PRE_WKS)
end   = wk0 + timedelta(weeks=POST_WKS+1)       # janela semiaberta [start, end)

# --- Base com semana por transaction_date ---
lf_tx_weeked = (
    lf_tx
    .with_columns(dt_expr_tx.alias("_dt_tx"))
    .with_columns(pl.col("_dt_tx").dt.truncate("1w").alias("week_tx"))
)

# (A) Janela semanal ao redor do pico — transaction_date
df_win_tx = (
    lf_tx_weeked
    .filter((pl.col("week_tx") >= pl.lit(start, dtype=pl.Datetime)) & (pl.col("week_tx") < pl.lit(end, dtype=pl.Datetime)))
    .group_by("week_tx")
    .agg([
        pl.len().alias("n_rows"),
        pl.col(qty_col).sum().alias("qty_sum"),
        pl.col(NET_COL).sum().alias("net_sum"),
        pl.col(GROSS_COL).sum().alias("gross_sum"),
        pl.col(PDV_COL).n_unique().alias("n_pdv"),
        pl.col(SKU_COL).n_unique().alias("n_sku"),
    ])
    .with_columns([
        pl.when(pl.col("qty_sum") > 0).then(pl.col("net_sum") / pl.col("qty_sum")).otherwise(None).alias("avg_price_net")
    ])
    .sort("week_tx")
    .collect()
)

# --- Base com semana por reference_date ---
lf_ref_weeked = (
    lf_tx
    .with_columns(dt_expr_ref.alias("_dt_ref"))
    .with_columns(pl.col("_dt_ref").dt.truncate("1w").alias("week_ref"))
)

# (B) Mesma janela semanal — reference_date (comparação)
df_win_ref = (
    lf_ref_weeked
    .filter((pl.col("week_ref") >= pl.lit(start, dtype=pl.Datetime)) & (pl.col("week_ref") < pl.lit(end, dtype=pl.Datetime)))
    .group_by("week_ref")
    .agg([
        pl.len().alias("n_rows"),
        pl.col(qty_col).sum().alias("qty_sum"),
        pl.col(NET_COL).sum().alias("net_sum"),
        pl.col(GROSS_COL).sum().alias("gross_sum"),
        pl.col(PDV_COL).n_unique().alias("n_pdv"),
        pl.col(SKU_COL).n_unique().alias("n_sku"),
    ])
    .with_columns([
        pl.when(pl.col("qty_sum") > 0).then(pl.col("net_sum") / pl.col("qty_sum")).otherwise(None).alias("avg_price_net")
    ])
    .sort("week_ref")
    .collect()
)

# (C) Breakdown DIÁRIO dentro da semana do pico (transaction_date)
df_daily_peak = (
    lf_tx_weeked
    .filter((pl.col("_dt_tx") >= pl.lit(wk0, dtype=pl.Datetime)) & (pl.col("_dt_tx") < pl.lit(wk0 + timedelta(days=7), dtype=pl.Datetime)))
    .with_columns(pl.col("_dt_tx").dt.truncate("1d").alias("day"))
    .group_by("day")
    .agg([
        pl.len().alias("n_rows"),
        pl.col(qty_col).sum().alias("qty_sum"),
        pl.col(NET_COL).sum().alias("net_sum"),
        pl.col(GROSS_COL).sum().alias("gross_sum"),
        pl.col(PDV_COL).n_unique().alias("n_pdv"),
        pl.col(SKU_COL).n_unique().alias("n_sku"),
    ])
    .with_columns([
        pl.when(pl.col("qty_sum") > 0).then(pl.col("net_sum") / pl.col("qty_sum")).otherwise(None).alias("avg_price_net")
    ])
    .sort("day")
    .collect()
)

print("=== (A) Janela semanal (transaction_date) — 4 antes, pico, 3 depois ===")
print(df_win_tx)

print("\n=== (B) Mesma janela (reference_date) — comparação ===")
print(df_win_ref)

print("\n=== (C) Breakdown diário na semana do pico (transaction_date) ===")
print(df_daily_peak)

# 05) Opção A — remover 2022-09-11 (transaction_date) e recalcular a série semanal (df_weekly)
import polars as pl
from datetime import datetime

# Pré-requisitos: lf_tx (LazyFrame), date_col (ex.: "transaction_date"), qty_col (ex.: "quantity")

# 1) Normaliza a coluna de data -> Datetime
schema = lf_tx.collect_schema()
dt_dtype = schema[date_col]
if dt_dtype in (pl.Date, pl.Datetime):
    dt_expr = pl.col(date_col).cast(pl.Datetime, strict=False)
elif dt_dtype == pl.Utf8:
    dt_expr = pl.col(date_col).str.strptime(pl.Datetime, strict=False, exact=False)
else:
    raise RuntimeError(f"dtype de {date_col} não suportado: {dt_dtype}. Esperado Date/Datetime/Utf8.")

TARGET_DAY = datetime(2022, 9, 11)

# 2) Quantifica o que será removido (dia 2022-09-11)
df_removed_stats = (
    lf_tx
    .with_columns(dt_expr.alias("_dt"))
    .with_columns(pl.col("_dt").dt.truncate("1d").alias("day"))
    .filter(pl.col("day") == pl.lit(TARGET_DAY, dtype=pl.Datetime))
    .select([
        pl.len().alias("n_rows_removed"),
        pl.col(qty_col).sum().alias("qty_removed")
    ])
    .collect()
)

n_rows_removed = int(df_removed_stats["n_rows_removed"][0]) if df_removed_stats.height else 0
qty_removed    = float(df_removed_stats["qty_removed"][0])   if df_removed_stats.height else 0.0
print(f"[Limpeza] Dia removido: {TARGET_DAY.date()} | linhas: {n_rows_removed} | qty_removida: {int(qty_removed)}")

# 3) Remove o dia e recalcula a série semanal
lf_tx_clean = (
    lf_tx
    .with_columns(dt_expr.alias("_dt"))
    .with_columns(pl.col("_dt").dt.truncate("1d").alias("day"))
    .filter(pl.col("day") != pl.lit(TARGET_DAY, dtype=pl.Datetime))
)

df_weekly = (
    lf_tx_clean
    .with_columns(pl.col("day").dt.truncate("1w").alias("week"))
    .group_by("week")
    .agg(pl.col(qty_col).sum().alias("qty_sum"))
    .sort("week")
    .collect()
)

print("[OK] Série semanal recalculada em df_weekly (sem 2022-09-11).")

# 06) Visualização após limpeza — linha (semanal) e histograma (qty_sum)

import matplotlib.pyplot as plt

# Checagens mínimas
assert "week" in df_weekly.columns and "qty_sum" in df_weekly.columns, "df_weekly deve conter colunas 'week' e 'qty_sum'."

# 1) Gráfico de linha — série semanal total
plt.figure(figsize=(7.5, 4))
plt.plot(df_weekly["week"].to_numpy(), df_weekly["qty_sum"].to_numpy(), linewidth=1.8)
plt.title("Vendas semanais (total, 2022) — após remover 2022-09-11")
plt.xlabel("Semana")
plt.ylabel("Quantidade (qty_sum)")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# 2) Histograma — distribuição de qty_sum semanal
plt.figure(figsize=(6.8, 3.8))
plt.hist(df_weekly["qty_sum"].to_numpy(), bins="auto")
plt.title("Distribuição de vendas semanais (qty_sum) — sem 2022-09-11")
plt.xlabel("Quantidade (qty_sum)")
plt.ylabel("Frequência")
plt.grid(True, axis="y", alpha=0.3)
plt.tight_layout()
plt.show()

# 07) Nível 2 — Sparsidade & zeros (PDV×SKU semanal)
# - Calcula semanas ativas, % de zeros, estatísticas por série (ativo), e lista Top-20 por volume e por variância
# Pré-requisitos: lf_tx (LazyFrame), date_col, qty_col. Se lf_tx_clean existir, usaremos como base.

import polars as pl
from datetime import datetime

# ---------- base de dados ----------
base = None
try:
    base = lf_tx_clean
except NameError:
    pass
if base is None:
    base = lf_tx
    # remove 2022-09-11 se ainda não foi limpo (mesmo critério da etapa A)
    schema0 = base.collect_schema()
    dt_dtype0 = schema0[date_col]
    if dt_dtype0 in (pl.Date, pl.Datetime):
        dt_expr0 = pl.col(date_col).cast(pl.Datetime, strict=False)
    elif dt_dtype0 == pl.Utf8:
        dt_expr0 = pl.col(date_col).str.strptime(pl.Datetime, strict=False, exact=False)
    else:
        raise RuntimeError(f"dtype de {date_col} não suportado: {dt_dtype0}")
    TARGET_DAY = datetime(2022, 9, 11)
    base = (
        base
        .with_columns(dt_expr0.alias("_dt"))
        .with_columns(pl.col("_dt").dt.truncate("1d").alias("day"))
        .filter(pl.col("day") != pl.lit(TARGET_DAY, dtype=pl.Datetime))
    )

# chaves reais
PDV_COL = "internal_store_id"
SKU_COL = "internal_product_id"

# ---------- determinar semanas totais ----------
# (usa df_weekly se existir; caso contrário, recalcula rápido a série semanal total)
try:
    weeks_total = df_weekly.height
    weeks_list = df_weekly["week"]
except NameError:
    # criar semana a partir de 'day' se existir; senão, a partir da coluna de data
    schema = base.collect_schema()
    if "day" in schema.names():
        lf_weeks = base.with_columns(pl.col("day").dt.truncate("1w").alias("week"))
    else:
        dt_dtype = schema[date_col]
        if dt_dtype in (pl.Date, pl.Datetime):
            dt_expr = pl.col(date_col).cast(pl.Datetime, strict=False)
        elif dt_dtype == pl.Utf8:
            dt_expr = pl.col(date_col).str.strptime(pl.Datetime, strict=False, exact=False)
        else:
            raise RuntimeError(f"dtype de {date_col} não suportado: {dt_dtype}")
        lf_weeks = (
            base
            .with_columns(dt_expr.alias("_dt"))
            .with_columns(pl.col("_dt").dt.truncate("1d").alias("day"))
            .with_columns(pl.col("day").dt.truncate("1w").alias("week"))
        )
    df_weeks = (
        lf_weeks
        .group_by("week")
        .agg(pl.col(qty_col).sum().alias("qty_sum"))
        .sort("week")
        .collect()
    )
    weeks_total = df_weeks.height
    weeks_list = df_weeks["week"]

# ---------- painel semanal PDV×SKU (só semanas com atividade) ----------
# sum por (PDV, SKU, week)
schema_b = base.collect_schema()
if "week" in schema_b.names():
    lf_weeked = base
else:
    if "day" in schema_b.names():
        lf_weeked = base.with_columns(pl.col("day").dt.truncate("1w").alias("week"))
    else:
        # cria 'day' e 'week' a partir do date_col
        dt_dtype = schema_b[date_col]
        if dt_dtype in (pl.Date, pl.Datetime):
            dt_expr = pl.col(date_col).cast(pl.Datetime, strict=False)
        elif dt_dtype == pl.Utf8:
            dt_expr = pl.col(date_col).str.strptime(pl.Datetime, strict=False, exact=False)
        else:
            raise RuntimeError(f"dtype de {date_col} não suportado: {dt_dtype}")
        lf_weeked = (
            base
            .with_columns(dt_expr.alias("_dt"))
            .with_columns(pl.col("_dt").dt.truncate("1d").alias("day"))
            .with_columns(pl.col("day").dt.truncate("1w").alias("week"))
        )

lf_pnl = (
    lf_weeked
    .group_by([PDV_COL, SKU_COL, "week"])
    .agg(pl.col(qty_col).sum().alias("qty_wk"))
)

# ---------- métricas por série (ativo) ----------
lf_series = (
    lf_pnl
    .group_by([PDV_COL, SKU_COL])
    .agg([
        pl.len().alias("weeks_active"),                   # nº de semanas com venda (>0)
        pl.col("qty_wk").sum().alias("qty_total"),
        pl.col("qty_wk").mean().alias("qty_mean_act"),
        pl.col("qty_wk").median().alias("qty_median_act"),
        pl.col("qty_wk").quantile(0.95, "nearest").alias("qty_p95_act"),
        pl.col("qty_wk").max().alias("qty_peak"),
        pl.col("qty_wk").var().alias("qty_var_act"),
    ])
    .with_columns([
        pl.lit(weeks_total).alias("weeks_total"),
        (pl.lit(weeks_total) - pl.col("weeks_active")).alias("weeks_zero"),
        ((pl.lit(weeks_total) - pl.col("weeks_active")) / pl.lit(weeks_total)).alias("zeros_pct"),
        pl.when(pl.col("qty_median_act") > 0)
          .then(pl.col("qty_peak") / pl.col("qty_median_act"))
          .otherwise(None)
          .alias("peak_to_median_act"),
    ])
)

# ---------- saídas principais ----------
# total de séries
n_series = lf_series.select(pl.len()).collect().item()

# Top-20 por volume total
df_top_volume = (
    lf_series
    .sort(["qty_total"], descending=[True])
    .limit(20)
    .collect()
)

# Top-20 por variância de volume
df_top_var = (
    lf_series
    .sort(["qty_var_act"], descending=[True])
    .limit(20)
    .collect()
)

print(f"Séries PDV×SKU: {n_series:,} | Semanas no ano (pós-limpeza): {weeks_total}".replace(",", "."))
print("\nTop-20 séries por volume total (qty_total):")
print(df_top_volume)

print("\nTop-20 séries por variância (qty_var_act):")
print(df_top_var)

# Observação:
# - zeros_pct = (semanas sem venda) / semanas_total (sem densificar matriz).
# - métricas *_act são calculadas sobre semanas ativas (com venda); zeros não entram nelas por eficiência.

# 08) Nível 2 — ADI & CV² por série (PDV×SKU), filtro weeks_active ≥ 3 e classificação S&BC
# Classes (Syntetos & Boylan):
#  - smooth:       ADI < 1.32  & CV² < 0.49
#  - intermittent: ADI ≥ 1.32  & CV² < 0.49
#  - erratic:      ADI < 1.32  & CV² ≥ 0.49
#  - lumpy:        ADI ≥ 1.32  & CV² ≥ 0.49
#
# Pré-requisitos: lf_series (da célula 07) contendo:
#  - PDV_COL="internal_store_id", SKU_COL="internal_product_id"
#  - weeks_active, weeks_total, qty_mean_act, qty_var_act, zeros_pct, etc.

import polars as pl

PDV_COL = "internal_store_id"
SKU_COL = "internal_product_id"

# Se lf_series não existir (rodagem isolada), levanta erro amigável
try:
    _ = lf_series
except NameError as _e:
    raise RuntimeError("lf_series não encontrado. Execute a célula 07 antes desta.")

# Calcula ADI, CV² e classifica; filtra weeks_active ≥ 3
lf_cls = (
    lf_series
    .with_columns([
        (pl.col("weeks_total") / pl.col("weeks_active")).alias("adi"),
        pl.when(pl.col("qty_mean_act") > 0)
          .then(pl.col("qty_var_act") / (pl.col("qty_mean_act") ** 2))
          .otherwise(None)
          .alias("cv2")
    ])
    .filter(pl.col("weeks_active") >= 3)
    .with_columns([
        pl.when((pl.col("adi") < 1.32) & (pl.col("cv2") < 0.49)).then(pl.lit("smooth"))
         .when((pl.col("adi") >= 1.32) & (pl.col("cv2") < 0.49)).then(pl.lit("intermittent"))
         .when((pl.col("adi") < 1.32) & (pl.col("cv2") >= 0.49)).then(pl.lit("erratic"))
         .otherwise(pl.lit("lumpy"))
         .alias("demand_class")
    ])
    .select([PDV_COL, SKU_COL, "weeks_active", "weeks_total", "zeros_pct", "adi", "cv2", "qty_mean_act", "qty_var_act", "demand_class"])
)

df_cls = lf_cls.collect()

# Distribuição por classe
total_series = df_cls.height
df_dist = (
    df_cls
    .group_by("demand_class")
    .agg(pl.len().alias("count"))
    .with_columns((pl.col("count") / total_series * 100).round(2).alias("pct"))
    .sort("count", descending=True)
)

print(f"Séries com weeks_active ≥ 3: {total_series:,}".replace(",", "."))
print("\nDistribuição por classe (S&B):")
print(df_dist)

# (Opcional para inspeção manual rápida: exemplos por classe)
for cls in ["smooth", "intermittent", "erratic", "lumpy"]:
    df_ex = df_cls.filter(pl.col("demand_class") == cls).head(5)
    if df_ex.height > 0:
        print(f"\nExemplos — {cls}:")
        print(df_ex)

# (1/4) PREP — agregação semanal + métricas de série (estável sem streaming em unique)

import polars as pl, os
from datetime import datetime

PDV_COL   = "internal_store_id"
SKU_COL   = "internal_product_id"
NET_COL   = "net_value"
GROSS_COL = "gross_value"
QTY_COL   = qty_col
DATE_COL  = date_col

os.makedirs("/content/tmp_ml", exist_ok=True)
WK_PATH    = "/content/tmp_ml/wk.parquet"
PAIRS_PATH = "/content/tmp_ml/pairs.parquet"
SER_PATH   = "/content/tmp_ml/series_feats.parquet"

# ===== Base limpa =====
TARGET_DAY = datetime(2022, 9, 11)
schema0 = lf_tx.collect_schema()
if DATE_COL not in schema0.names():
    raise RuntimeError(f"Coluna de data não encontrada: {DATE_COL}")

if schema0[DATE_COL] in (pl.Date, pl.Datetime):
    dt_expr0 = pl.col(DATE_COL).cast(pl.Datetime, strict=False)
elif schema0[DATE_COL] == pl.Utf8:
    dt_expr0 = pl.col(DATE_COL).str.strptime(pl.Datetime, strict=False, exact=False)
else:
    raise RuntimeError(f"dtype de {DATE_COL} não suportado: {schema0[DATE_COL]}")

base = (
    lf_tx
    .with_columns(dt_expr0.alias("_dt"))
    .with_columns(pl.col("_dt").dt.truncate("1d").alias("day"))
    .filter(pl.col("day") != pl.lit(TARGET_DAY, dtype=pl.Datetime))
    .with_columns([
        pl.col(PDV_COL).cast(pl.Categorical),
        pl.col(SKU_COL).cast(pl.Categorical),
        pl.col(QTY_COL).cast(pl.Float32),
        pl.col(NET_COL).cast(pl.Float32),
        pl.col(GROSS_COL).cast(pl.Float32),
    ])
)

# ===== Agrega semanal =====
lf_weeked = (
    base
    .with_columns(pl.col("_dt").dt.truncate("1w").alias("week"))
    .group_by([PDV_COL, SKU_COL, "week"])
    .agg([
        pl.col(QTY_COL).sum().alias("qty_wk"),
        pl.col(NET_COL).sum().alias("net_sum"),
        pl.col(GROSS_COL).sum().alias("gross_sum"),
    ])
)

# Coleta em streaming (seguro aqui) e grava
df_weeked = lf_weeked.collect(streaming=True)
df_weeked.write_parquet(WK_PATH)
lf_week = pl.scan_parquet(WK_PATH)

# ===== Semanas de 2022 =====
df_weeks = (
    lf_week.select("week").unique().filter(pl.col("week").dt.year()==2022).sort("week").collect()
)
weeks_valid = df_weeks["week"].tail(4).to_list()

# ===== PARES (sem streaming; e IDs como Utf8 para evitar bug) =====
pairs_df = (
    df_weeked
    .select([PDV_COL, SKU_COL])
    .with_columns([
        pl.col(PDV_COL).cast(pl.Utf8),
        pl.col(SKU_COL).cast(pl.Utf8),
    ])
    .unique(maintain_order=False)
)
pairs_df.write_parquet(PAIRS_PATH)

# ===== Métricas por série (saída pequena; sem streaming) =====
weeks_total = df_weeks.height
df_ser = (
    lf_week
    .group_by([PDV_COL, SKU_COL])
    .agg([
        pl.count().alias("weeks_active"),
        pl.col("qty_wk").sum().alias("qty_total"),
        pl.col("qty_wk").mean().alias("qty_mean_act"),
        pl.col("qty_wk").var().alias("qty_var_act"),
    ])
    .with_columns([
        pl.lit(weeks_total).alias("weeks_total"),
        ((pl.lit(weeks_total) - pl.col("weeks_active"))/pl.lit(weeks_total)).alias("zeros_pct"),
        (pl.lit(weeks_total) / pl.col("weeks_active")).alias("adi"),
        pl.when(pl.col("qty_mean_act") > 0)
          .then(pl.col("qty_var_act") / (pl.col("qty_mean_act")**2))
          .otherwise(None)
          .alias("cv2"),
    ])
    .with_columns(
        pl.when((pl.col("adi") < 1.32) & (pl.col("cv2") < 0.49)).then(pl.lit("smooth"))
         .when((pl.col("adi") >= 1.32) & (pl.col("cv2") < 0.49)).then(pl.lit("intermittent"))
         .when((pl.col("adi") < 1.32) & (pl.col("cv2") >= 0.49)).then(pl.lit("erratic"))
         .otherwise(pl.lit("lumpy")).alias("demand_class")
    )
    .collect()  # sem streaming aqui
)
df_ser.write_parquet(SER_PATH)

print("OK | wk:", WK_PATH, "| pairs:", PAIRS_PATH, "| series:", SER_PATH)
print("weeks_valid:", weeks_valid)

# (2/4) DATASET — gerar features por LOTE (bucket) + downsample de zeros e salvar parquet de treino
import polars as pl, os

WK_PATH    = "/content/tmp_ml/wk.parquet"
PAIRS_PATH = "/content/tmp_ml/pairs.parquet"
SER_PATH   = "/content/tmp_ml/series_feats.parquet"
OUT_DIR    = "/content/tmp_ml/train_batches"
os.makedirs(OUT_DIR, exist_ok=True)

BATCHES    = 20
NEG_FRAC   = 0.10
US_PER_WEEK= 7*24*3600*1_000_000

PDV_COL = "internal_store_id"
SKU_COL = "internal_product_id"

lf_week  = pl.scan_parquet(WK_PATH)
lf_pairs = pl.scan_parquet(PAIRS_PATH)
lf_ser   = pl.scan_parquet(SER_PATH)

# 🔧 Normaliza tipos das chaves (evita mismatch cat/str nos joins)
KEY_CAST = [
    pl.col(PDV_COL).cast(pl.Utf8).alias(PDV_COL),
    pl.col(SKU_COL).cast(pl.Utf8).alias(SKU_COL),
]
lf_week  = lf_week.with_columns(KEY_CAST)
lf_pairs = lf_pairs.with_columns(KEY_CAST)
lf_ser   = lf_ser.with_columns(KEY_CAST)

# Semanas de 2022 (Lazy)
weeks_lf = pl.DataFrame({"week": lf_week.select("week").unique().collect()["week"]}).lazy()
weeks_valid = weeks_lf.select("week").sort("week").collect()["week"].tail(4).to_list()

FEATS = [
    "recency_weeks", "weeks_active_cum", "intensity_cum",
    "avg_price_net", "discount_share", "weekofyear",
    "adi", "cv2", "zeros_pct",
    "cls_smooth", "cls_intermittent", "cls_erratic", "cls_lumpy",
]

for b in range(BATCHES):
    # pares do lote b (hash mod)
    pairs_b = lf_pairs.filter(
        (pl.struct([PDV_COL, SKU_COL]).hash(seed=0) % pl.lit(BATCHES)) == pl.lit(b)
    )

    # grade do lote (pares_b × semanas_2022)
    grid_b = pairs_b.join(weeks_lf, how="cross")

    # junta semanal (left) → faltas = zero
    pnl_b = (
        grid_b
        .join(lf_week, on=[PDV_COL, SKU_COL, "week"], how="left")
        .with_columns([
            pl.col("qty_wk").fill_null(0.0).cast(pl.Float32),
            pl.col("net_sum").fill_null(0.0).cast(pl.Float32),
            pl.col("gross_sum").fill_null(0.0).cast(pl.Float32),
        ])
        .sort([PDV_COL, SKU_COL, "week"])
        .with_columns([
            (pl.col("qty_wk") > 0).cast(pl.Int8).alias("sold"),
            pl.when(pl.col("qty_wk") > 0).then(pl.col("qty_wk")).otherwise(0.0).alias("_qty_pos"),
        ])
    )

    # cumulativos/recency
    pnl_b = (
        pnl_b
        .with_columns([
            pl.when(pl.col("sold")==1).then(pl.col("week")).otherwise(None)
              .forward_fill().over([PDV_COL, SKU_COL]).alias("_last_sale_week"),
            pl.col("sold").cum_sum().over([PDV_COL, SKU_COL]).alias("_cum_sold"),
            pl.col("_qty_pos").cum_sum().over([PDV_COL, SKU_COL]).alias("_cum_qty"),
        ])
        .with_columns([
            ((pl.col("week").cast(pl.Int64) - pl.col("_last_sale_week").cast(pl.Int64)) // pl.lit(US_PER_WEEK, dtype=pl.Int64))
              .fill_null(99).cast(pl.Int16).alias("recency_weeks"),
            pl.col("_cum_sold").cast(pl.Int16).alias("weeks_active_cum"),
            pl.when(pl.col("_cum_sold") > 0).then(pl.col("_cum_qty")/pl.col("_cum_sold")).otherwise(0.0).cast(pl.Float32).alias("intensity_cum"),
            pl.when(pl.col("qty_wk") > 0).then(pl.col("net_sum")/pl.col("qty_wk")).otherwise(None).cast(pl.Float32).alias("avg_price_net"),
            pl.when(pl.col("gross_sum") > 0).then(1 - (pl.col("net_sum")/pl.col("gross_sum"))).otherwise(None).cast(pl.Float32).alias("discount_share"),
            pl.col("week").dt.week().cast(pl.Int16).alias("weekofyear"),
        ])
    )

    # métricas de série + one-hots
    pnl_b = pnl_b.join(
        lf_ser.select([PDV_COL, SKU_COL, "adi", "cv2", "zeros_pct", "demand_class"]),
        on=[PDV_COL, SKU_COL], how="left"
    )
    for c in ["smooth", "intermittent", "erratic", "lumpy"]:
        pnl_b = pnl_b.with_columns(
            pl.when(pl.col("demand_class")==c).then(1).otherwise(0).cast(pl.Int8).alias(f"cls_{c}")
        )

    # treino = fora das 4 últimas semanas
    pnl_train_b = (
        pnl_b
        .filter(~pl.col("week").is_in(weeks_valid))
        .select([PDV_COL, SKU_COL, "week", "qty_wk", "sold"] + FEATS)
        .collect(streaming=True)  # coleta por lote
    )

    # downsample de zeros (global no lote)
    df_pos = pnl_train_b.filter(pl.col("sold")==1)
    df_neg = pnl_train_b.filter(pl.col("sold")==0)
    if df_neg.height > 0 and NEG_FRAC < 1.0:
        df_neg = df_neg.sample(fraction=NEG_FRAC, with_replacement=False, shuffle=True, seed=42)
    df_train_b = pl.concat([df_pos, df_neg], how="vertical")

    outp = os.path.join(OUT_DIR, f"train_batch_{b:02d}.parquet")
    df_train_b.write_parquet(outp)
    print(f"[batch {b+1}/{BATCHES}] -> {outp} | pos={df_pos.height:,} neg_samp={df_neg.height:,}".replace(",", "."))

print("[DATASET] FEITO")

# (3/4) TREINO — Head 1 (classificação) e Head 2 (regressão condicional) a partir dos batches
import polars as pl, glob
import numpy as np, pandas as pd
from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor

TRAIN_FILES = sorted(glob.glob("/content/tmp_ml/train_batches/train_batch_*.parquet"))
assert TRAIN_FILES, "Nenhum batch de treino encontrado."

FEATS = [
    "recency_weeks", "weeks_active_cum", "intensity_cum",
    "avg_price_net", "discount_share", "weekofyear",
    "adi", "cv2", "zeros_pct",
    "cls_smooth", "cls_intermittent", "cls_erratic", "cls_lumpy",
]

# Carrega e concatena só as colunas necessárias
df_train_all = pl.scan_parquet(TRAIN_FILES).select(FEATS + ["sold", "qty_wk"]).collect(streaming=True)

# Head 1 — Classificação
X_cls = df_train_all.select(FEATS).to_pandas()
y_cls = df_train_all["sold"].to_pandas().astype(int)

clf = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.1, random_state=42)
clf.fit(X_cls, y_cls)

# Head 2 — Regressão condicional (apenas vendidos)
df_reg = df_train_all.filter(pl.col("sold")==1).with_columns(pl.col("qty_wk").log1p().alias("y_log"))
X_reg = df_reg.select(FEATS).to_pandas()
y_reg = df_reg["y_log"].to_pandas().astype(np.float32)

reg = HistGradientBoostingRegressor(max_iter=400, learning_rate=0.06, random_state=42)
reg.fit(X_reg, y_reg)

print(f"[TRAIN] clf: n={len(y_cls)} | pos={int(y_cls.sum())} | neg={len(y_cls)-int(y_cls.sum())}")
print(f"[TRAIN] reg: n={len(y_reg)} (semanas com venda)")

# (4/4) PREVISÃO & MÉTRICAS — últimas 4 semanas de 2022 (processamento por lote)
import polars as pl, os, numpy as np

WK_PATH    = "/content/tmp_ml/wk.parquet"
PAIRS_PATH = "/content/tmp_ml/pairs.parquet"
SER_PATH   = "/content/tmp_ml/series_feats.parquet"
OUT_DIR    = "/content/tmp_ml/preds"
os.makedirs(OUT_DIR, exist_ok=True)

BATCHES    = 20
US_PER_WEEK= 7*24*3600*1_000_000

PDV_COL = "internal_store_id"
SKU_COL = "internal_product_id"

lf_week  = pl.scan_parquet(WK_PATH)
lf_pairs = pl.scan_parquet(PAIRS_PATH)
lf_ser   = pl.scan_parquet(SER_PATH)

# normaliza tipos das chaves
KEY_CAST = [
    pl.col(PDV_COL).cast(pl.Utf8).alias(PDV_COL),
    pl.col(SKU_COL).cast(pl.Utf8).alias(SKU_COL),
]
lf_week  = lf_week.with_columns(KEY_CAST)
lf_pairs = lf_pairs.with_columns(KEY_CAST)
lf_ser   = lf_ser.with_columns(KEY_CAST)

# semanas de 2022 e janela de validação
weeks_all = (
    lf_week.select("week").unique()
    .filter(pl.col("week").dt.year() == 2022)
    .sort("week").collect()["week"]
)
weeks_valid = weeks_all[-4:]
weeks_lf = pl.DataFrame({"week": weeks_all}).lazy()

FEATS = [
    "recency_weeks", "weeks_active_cum", "intensity_cum",
    "avg_price_net", "discount_share", "weekofyear",
    "adi", "cv2", "zeros_pct",
    "cls_smooth", "cls_intermittent", "cls_erratic", "cls_lumpy",
]

pred_files = []
for b in range(BATCHES):
    # pares do lote (sem coluna auxiliar)
    pairs_b = lf_pairs.filter(
        (pl.struct([PDV_COL, SKU_COL]).hash(seed=0) % pl.lit(BATCHES)) == pl.lit(b)
    )

    # grade (pares × semanas)
    grid_b = pairs_b.join(weeks_lf, how="cross")

    # painel + features
    pnl_b = (
        grid_b
        .join(lf_week, on=[PDV_COL, SKU_COL, "week"], how="left")
        .with_columns([
            pl.col("qty_wk").fill_null(0.0).cast(pl.Float32),
            pl.col("net_sum").fill_null(0.0).cast(pl.Float32),
            pl.col("gross_sum").fill_null(0.0).cast(pl.Float32),
        ])
        .sort([PDV_COL, SKU_COL, "week"])
        .with_columns([
            (pl.col("qty_wk") > 0).cast(pl.Int8).alias("sold"),
            pl.when(pl.col("qty_wk") > 0).then(pl.col("qty_wk")).otherwise(0.0).alias("_qty_pos"),
        ])
        .with_columns([
            pl.when(pl.col("sold")==1).then(pl.col("week")).otherwise(None)
              .forward_fill().over([PDV_COL, SKU_COL]).alias("_last_sale_week"),
            pl.col("sold").cum_sum().over([PDV_COL, SKU_COL]).alias("_cum_sold"),
            pl.col("_qty_pos").cum_sum().over([PDV_COL, SKU_COL]).alias("_cum_qty"),
        ])
        .with_columns([
            ((pl.col("week").cast(pl.Int64) - pl.col("_last_sale_week").cast(pl.Int64)) // pl.lit(US_PER_WEEK, dtype=pl.Int64))
              .fill_null(99).cast(pl.Int16).alias("recency_weeks"),
            pl.col("_cum_sold").cast(pl.Int16).alias("weeks_active_cum"),
            pl.when(pl.col("_cum_sold") > 0).then(pl.col("_cum_qty")/pl.col("_cum_sold")).otherwise(0.0).cast(pl.Float32).alias("intensity_cum"),
            pl.when(pl.col("qty_wk") > 0).then(pl.col("net_sum")/pl.col("qty_wk")).otherwise(None).cast(pl.Float32).alias("avg_price_net"),
            pl.when(pl.col("gross_sum") > 0).then(1 - (pl.col("net_sum")/pl.col("gross_sum"))).otherwise(None).cast(pl.Float32).alias("discount_share"),
            pl.col("week").dt.week().cast(pl.Int16).alias("weekofyear"),
        ])
        .join(
            lf_ser.select([PDV_COL, SKU_COL, "adi", "cv2", "zeros_pct", "demand_class"]),
            on=[PDV_COL, SKU_COL], how="left"
        )
    )
    for c in ["smooth", "intermittent", "erratic", "lumpy"]:
        pnl_b = pnl_b.with_columns(
            pl.when(pl.col("demand_class")==c).then(1).otherwise(0).cast(pl.Int8).alias(f"cls_{c}")
        )

    # mantém apenas as 4 últimas semanas
    df_val_b = (
        pnl_b
        .filter(pl.col("week").is_in(weeks_valid))
        .select([PDV_COL, SKU_COL, "week", "qty_wk", "sold", "demand_class"] + FEATS)
        .collect(streaming=True)
    )

    # predição (modelos já treinados: clf e reg)
    X = df_val_b.select(FEATS).to_pandas()
    p_hat = clf.predict_proba(X)[:, 1]
    ylog_hat = reg.predict(X)
    qty_cond = np.expm1(ylog_hat)
    pred_qty = p_hat * qty_cond

    # adiciona colunas (em duas etapas para não referenciar coluna recém-criada)
    df_out = df_val_b.with_columns([
        pl.Series("p_hat", p_hat),
        pl.Series("qty_cond_hat", qty_cond),
        pl.Series("pred_qty", pred_qty),
    ])
    df_out = df_out.with_columns(
        (pl.col("pred_qty") - pl.col("qty_wk")).abs().alias("abs_err")
    )

    outp = f"{OUT_DIR}/pred_batch_{b:02d}.parquet"
    df_out.write_parquet(outp)
    pred_files.append(outp)
    print(f"[pred {b+1}/{BATCHES}] -> {outp} ({df_out.height:,} linhas)".replace(",", "."))

# agrega previsões
df_pred_all = pl.scan_parquet(f"{OUT_DIR}/pred_batch_*.parquet").collect(streaming=True)

# métricas gerais
total_abs_err = float(df_pred_all["abs_err"].sum())
denom_qty     = float(df_pred_all["qty_wk"].sum())
WAPE = total_abs_err / max(denom_qty, 1.0)
MAE  = float(df_pred_all["abs_err"].mean())
print(f"\nWAPE (geral): {WAPE:.4f} | MAE: {MAE:.4f}")

# métricas por classe
if "demand_class" in df_pred_all.columns:
    df_cls = (
        df_pred_all
        .group_by("demand_class")
        .agg([
            pl.col("abs_err").sum().alias("abs_err_sum"),
            pl.col("qty_wk").sum().alias("qty_sum"),
            pl.col("abs_err").mean().alias("MAE"),
        ])
        .with_columns(
            (pl.col("abs_err_sum") / pl.when(pl.col("qty_sum") > 0).then(pl.col("qty_sum")).otherwise(1.0)).alias("WAPE")
        )
        .select(["demand_class", "WAPE", "MAE"])
        .sort("WAPE")
    )
    print("\nWAPE/MAE por classe:")
    print(df_cls)

# R² (global, por classe e por horizonte) a partir das previsões salvas
import polars as pl

PRED_GLOB = "/content/tmp_ml/preds/pred_batch_*.parquet"

df = (
    pl.scan_parquet(PRED_GLOB)
      .select([
          "internal_store_id", "internal_product_id", "week",
          "qty_wk", "pred_qty", "demand_class",
          pl.col("p_hat").alias("_p_hat"), pl.col("qty_cond_hat").alias("_qty_cond_hat"),
      ])
      .collect(streaming=True)
)

# pred usado: coalesce(pred_qty, p_hat*qty_cond_hat) e clip a >= 0
df = df.with_columns([
    pl.coalesce([
        pl.col("pred_qty"),
        (pl.col("_p_hat") * pl.col("_qty_cond_hat"))
    ]).fill_null(0.0).clip(lower_bound=0.0).alias("_pred")
])

# --- R² GLOBAL ---
ybar = df.select(pl.col("qty_wk").mean().alias("_ybar")).item()
SSE  = df.select(((pl.col("qty_wk") - pl.col("_pred"))**2).sum().alias("SSE")).item()
SST  = df.select(((pl.col("qty_wk") - ybar)**2).sum().alias("SST")).item()
R2_global = 1.0 - (SSE / max(SST, 1e-12))
print(f"R² (global): {R2_global:.6f}")

# --- R² POR CLASSE ---
if "demand_class" in df.columns:
    df_tmp = df.with_columns(
        pl.col("qty_wk").mean().over("demand_class").alias("_ybar_cls")
    )
    df_cls_r2 = (
        df_tmp.with_columns([
            (pl.col("qty_wk") - pl.col("_pred")).pow(2).alias("_sq_err"),
            (pl.col("qty_wk") - pl.col("_ybar_cls")).pow(2).alias("_sq_tot_cls"),
        ])
        .group_by("demand_class")
        .agg([
            pl.col("_sq_err").sum().alias("SSE"),
            pl.col("_sq_tot_cls").sum().alias("SST"),
        ])
        .with_columns(
            (1 - (pl.col("SSE") / pl.when(pl.col("SST") > 0).then(pl.col("SST")).otherwise(1.0))).alias("R2")
        )
        .select(["demand_class", "R2"])
        .sort("R2", descending=True)
    )
    print("\nR² por classe:")
    print(df_cls_r2)

# --- R² POR HORIZONTE (1..4) ---
weeks_sorted = df.select("week").unique().sort("week").to_series()
weeks_valid  = weeks_sorted.tail(4).to_list()
h_map = pl.DataFrame({"week": weeks_valid, "h": [1, 2, 3, 4]})

df_h_tmp = df.join(h_map, on="week", how="inner").with_columns(
    pl.col("qty_wk").mean().over("h").alias("_ybar_h")
)
df_h = (
    df_h_tmp.with_columns([
        (pl.col("qty_wk") - pl.col("_pred")).pow(2).alias("_sq_err"),
        (pl.col("qty_wk") - pl.col("_ybar_h")).pow(2).alias("_sq_tot_h"),
    ])
    .group_by("h")
    .agg([
        pl.col("_sq_err").sum().alias("SSE"),
        pl.col("_sq_tot_h").sum().alias("SST"),
    ])
    .with_columns(
        (1 - (pl.col("SSE") / pl.when(pl.col("SST") > 0).then(pl.col("SST")).otherwise(1.0))).alias("R2")
    )
    .select(["h", "R2"])
    .sort("h")
)
print("\nR² por horizonte (1=primeira semana da janela, 4=última):")
print(df_h)

# --- (Opcional) R² no log1p ---
df_log = df.select([
    pl.col("qty_wk").log1p().alias("_y_log"),
    pl.col("_pred").log1p().alias("_yhat_log"),
])
ybar_log = df_log.select(pl.col("_y_log").mean()).item()
SSE_log  = df_log.select(((pl.col("_y_log") - pl.col("_yhat_log"))**2).sum()).item()
SST_log  = df_log.select(((pl.col("_y_log") - ybar_log)**2).sum()).item()
R2_log   = 1.0 - (SSE_log / max(SST_log, 1e-12))
print(f"\nR² (global) no log1p: {R2_log:.6f}")

# ==== A) SPLITS E MAPA DE HORIZONTE (usa seu WK_PATH) ====
import polars as pl

STORE_COL, SKU_COL, QTY_COL = "internal_store_id", "internal_product_id", "qty_wk"
WK_PATH   = "/content/tmp_ml/wk.parquet"
TRAIN_DIR = "/content/tmp_ml/train_batches"    # batches com features
PRED_DIR  = "/content/tmp_ml/preds"

lf_week = pl.scan_parquet(WK_PATH)
weeks_2022 = (lf_week.select("week").unique()
                      .filter(pl.col("week").dt.year()==2022)
                      .sort("week").collect()["week"])
weeks_last4 = weeks_2022[-4:]
weeks_val4  = weeks_2022[-8:-4]    # para calibração/validação do regressor
weeks_train = set(weeks_2022[:-8])  # base do treino do regressor

# mapa horizonte (1..4) para 2022
h_map = pl.DataFrame({"week": weeks_last4, "h": [1,2,3,4]})
h_map_val = pl.DataFrame({"week": weeks_val4, "h": [1,2,3,4]})
print("Semanas: train ≤", weeks_2022[-9], "| val_prev4:", weeks_val4, "| test_last4:", weeks_last4)

# ==== B) LOAD DOS BATCHES + FILTROS ====
import glob, os

DEFAULT_FEATS = [
    "recency_weeks","weeks_active_cum","intensity_cum",
    "avg_price_net","discount_share","weekofyear",
    "adi","cv2","zeros_pct",
    "cls_smooth","cls_intermittent","cls_erratic","cls_lumpy",
    "lag_qty_1","lag_qty_4","lag_qty_8",
    "woy_sin_1","woy_sin_2","woy_sin_3","woy_cos_1","woy_cos_2","woy_cos_3",
]

batches = sorted(glob.glob(os.path.join(TRAIN_DIR, "train_batch_*.parquet")))
if not batches:
    raise RuntimeError("Gere os train_batches com lags/Fourier antes.")

# Unifica schema e mantém só o necessário
schema0 = pl.scan_parquet(batches[0]).collect_schema().names()
FEATS_ALL = [c for c in DEFAULT_FEATS if c in schema0]
need_cols = [STORE_COL, SKU_COL, "week", QTY_COL, "sold", "demand_class"] + FEATS_ALL

lf_all = pl.concat(
    [pl.scan_parquet(f).select([c for c in need_cols if c in pl.scan_parquet(f).collect_schema().names()])
     for f in batches], how="vertical"
)

# Prepara dataset POSITIVO para o regressor
df_all = lf_all.collect(streaming=True)
df_pos = df_all.filter(pl.col("sold")==1)

# Adiciona horizonte: val e test
df_pos = df_pos.join(h_map_val, on="week", how="left").join(h_map, on="week", how="left", suffix="_test")
df_pos = df_pos.with_columns([
    pl.coalesce([pl.col("h"), pl.col("h_test")]).alias("h")  # usa h do conjunto ao qual a semana pertence (val ou test)
]).drop(["h_test"], strict=False)

# Splits do regressor
df_reg_train = df_pos.filter(pl.col("week").is_in(list(weeks_train)))
df_reg_val   = df_pos.filter(pl.col("week").is_in(weeks_val4))
print(df_reg_train.shape, df_reg_val.shape, "features:", len(FEATS_ALL))

# ==== C) REGRESSORES POR (classe × h) — LightGBM em log1p ====
import os, joblib
import numpy as np
import lightgbm as lgb
import polars as pl  # garante import

os.makedirs(PRED_DIR, exist_ok=True)
KEY = (STORE_COL, SKU_COL, "week")

def fit_one(df_tr: pl.DataFrame, df_va: pl.DataFrame, feats):
    Xtr = df_tr.select(feats).to_pandas().astype(np.float32)
    ytr = np.log1p(df_tr[QTY_COL].to_pandas().astype(np.float32))  # log1p
    wtr = np.clip(df_tr[QTY_COL].to_pandas().values, 1.0, None).astype(np.float32)  # peso leve

    Xva = df_va.select(feats).to_pandas().astype(np.float32)
    yva = np.log1p(df_va[QTY_COL].to_pandas().astype(np.float32))
    wva = np.clip(df_va[QTY_COL].to_pandas().values, 1.0, None).astype(np.float32)

    model = lgb.LGBMRegressor(
        objective="regression_l2",
        learning_rate=0.05, n_estimators=2000,
        num_leaves=63, min_child_samples=80,
        subsample=0.8, colsample_bytree=0.8,
        reg_lambda=1.0, random_state=42, n_jobs=-1,
        # force_col_wise=True  # habilite se RAM apertar
    )
    model.fit(
        Xtr, ytr,
        sample_weight=wtr,
        eval_set=[(Xva, yva)],
        eval_sample_weight=[wva],
        eval_metric="l2",
        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],
    )
    return model

models = {}
classes = ["smooth","intermittent","erratic","lumpy"]
for cls in classes:
    cls_col = f"cls_{cls}"
    for h in [1, 2, 3, 4]:
        # treino: semanas históricas (em df_reg_train, 'h' deve ser nulo)
        tr = df_reg_train.filter((pl.col(cls_col) == 1) & pl.col("h").is_null())
        # validação: as 4 semanas anteriores às últimas 4 (df_reg_val já tem h=1..4)
        va = df_reg_val.filter((pl.col(cls_col) == 1) & (pl.col("h") == h))

        if tr.height == 0 or va.height == 0:
            print(f"[skip] {cls} h={h}: tr={tr.height}, va={va.height}")
            continue

        m = fit_one(tr, va, FEATS_ALL)
        models[(cls, h)] = m
        outp = os.path.join(PRED_DIR, f"reg_log_{cls}_h{h}.joblib")
        joblib.dump(m, outp)
        print(f"[OK] reg_log_{cls}_h{h}  best_iter={m.best_iteration_}  | tr={tr.height:,} va={va.height:,}".replace(",", "."))
print("Total modelos:", len(models))

# ==== D) CALIBRAÇÃO DE ESCALA a*(exp(yhat)-1) por (classe×h) ====
import os, numpy as np, joblib
import polars as pl

def slope_calib(y, yhat, lo=0.90, hi=1.30):
    # a* = argmin ||y - a*yhat||²  → solução fechada
    num = float((yhat * y).sum())
    den = float((yhat * yhat).sum()) + 1e-12
    a = num / den
    return float(np.clip(a, lo, hi))

classes = ["smooth","intermittent","erratic","lumpy"]
calib = {}

for cls in classes:
    cls_col = f"cls_{cls}"
    for h in [1, 2, 3, 4]:
        # fatia de validação do grupo (classe × horizonte)
        va = df_reg_val.filter((pl.col(cls_col) == 1) & (pl.col("h") == h))
        if va.height == 0:
            print(f"[skip] {cls} h={h}: sem linhas na validação")
            continue

        # carrega modelo treinado para o grupo
        mpath = os.path.join(PRED_DIR, f"reg_log_{cls}_h{h}.joblib")
        if not os.path.exists(mpath):
            print(f"[skip] {cls} h={h}: modelo não encontrado em {mpath}")
            continue
        m = joblib.load(mpath)

        # features e alvo da validação
        Xva = va.select(FEATS_ALL).to_pandas().astype(np.float32)
        y_true = va[QTY_COL].to_pandas().to_numpy(dtype=np.float32)
        sold   = va["sold"].to_pandas().to_numpy(dtype=np.int8)

        # previsões em quantidade (desfaz log1p) e trunca a zero
        yhat_qty = np.expm1(m.predict(Xva, num_iteration=m.best_iteration_)).astype(np.float32)
        yhat_qty = np.clip(yhat_qty, 0.0, None)

        # calibra slope usando apenas positivos; se não houver, usa todas
        mask_pos = sold == 1
        if mask_pos.any():
            a = slope_calib(y_true[mask_pos], yhat_qty[mask_pos], lo=0.90, hi=1.30)
        else:
            a = slope_calib(y_true, yhat_qty, lo=0.90, hi=1.30)

        calib[(cls, h)] = a
        print(f"calib {cls}, h{h}: a={a:.3f}  | n_val={va.height:,}".replace(",", "."))

# salva dicionário de calibração
joblib.dump(calib, os.path.join(PRED_DIR, "calib_cls_h.joblib"))
print("[OK] calibrações salvas em calib_cls_h.joblib")

# ==== E) INFERÊNCIA (4 últimas semanas) — p_hat × qtŷ_calibrada (corrigida & RAM-friendly) ====
import os, glob, math
import numpy as np
import polars as pl
import joblib

# --- Parâmetros/constantes que já existem na pipeline ---
# STORE_COL, SKU_COL, QTY_COL
# WK_PATH, SER_PATH, TRAIN_DIR, PRED_DIR
# FEATS_ALL (features da regressão), classes = ["smooth","intermittent","erratic","lumpy"]

os.makedirs(PRED_DIR, exist_ok=True)

# 1) Definir semanas e mapa de horizonte (h=1..4)
lf_week = pl.scan_parquet(WK_PATH)
weeks_all = (
    lf_week.select("week").unique().filter(pl.col("week").dt.year() == 2022)
           .sort("week").collect(streaming=True)["week"]
)
weeks_last4 = weeks_all[-4:]
h_map = pl.DataFrame({"week": weeks_last4, "h": [1, 2, 3, 4]})

# 2) Carregar base de teste a partir dos batches (mesmo schema/feats do treino)
train_files = sorted(glob.glob(os.path.join(TRAIN_DIR, "train_batch_*.parquet")))
if not train_files:
    raise RuntimeError("Não encontrei batches em TRAIN_DIR. Gere os batches (P2) antes.")

# nem todos os arquivos podem ter todas as features; normaliza preenchendo faltantes com 0
feats_required = FEATS_ALL[:]  # cópia
schema0 = pl.scan_parquet(train_files[0]).collect_schema().names()
feats_required = [c for c in feats_required if c in set(schema0)]  # interseção por segurança

lfs = []
for f in train_files:
    lf = pl.scan_parquet(f)
    have = set(lf.collect_schema().names())
    miss = [c for c in feats_required if c not in have]
    if miss:
        lf = lf.with_columns([pl.lit(0.0).alias(c) for c in miss])
    lfs.append(lf.select([STORE_COL, SKU_COL, "week", QTY_COL, "sold"] + feats_required +
                         ["cls_smooth","cls_intermittent","cls_erratic","cls_lumpy"]))
df_all = pl.concat(lfs, how="vertical").collect(streaming=True)

# mantém só as 4 últimas semanas e adiciona h
df_test = (
    df_all.filter(pl.col("week").is_in(weeks_last4))
          .join(h_map, on="week", how="left")
          # garante dtypes leves e sem null nas features
          .with_columns([pl.col(c).cast(pl.Float32).fill_null(0.0).alias(c) for c in feats_required])
)

# 3) p_hat (classificador)
#    a) tenta clf.joblib; b) tenta pegar dos preds v1; c) fallback proxy (sold>0) só p/ não quebrar
p_hat = None
clf_path = os.path.join(PRED_DIR, "clf.joblib")
if os.path.exists(clf_path):
    clf = joblib.load(clf_path)
    # Se você treinou o classificador com um conjunto diferente de features (ex.: FEATS_CLS), defina-o antes.
    FEATS_CLS = feats_required  # ajuste aqui se tiver lista específica para o classificador
    Xc = df_test.select(FEATS_CLS).to_pandas().astype(np.float32)
    p_hat = clf.predict_proba(Xc)[:, 1].astype(np.float32)
else:
    # tenta buscar p_hat dos preds v1
    old_preds = sorted(glob.glob(os.path.join(PRED_DIR, "pred_batch_*.parquet")))
    if old_preds:
        df_p = (
            pl.scan_parquet(old_preds)
              .filter(pl.col("week").is_in(weeks_last4))
              .select([STORE_COL, SKU_COL, "week", "p_hat"])
              .collect(streaming=True)
        )
        df_test = df_test.join(df_p, on=[STORE_COL, SKU_COL, "week"], how="left")
        if "p_hat" in df_test.columns:
            p_hat = df_test["p_hat"].fill_null(0.0).to_numpy().astype(np.float32)

if p_hat is None:
    print("⚠️ p_hat não encontrado. Usando proxy 1*(sold>0) apenas para não quebrar (substitua pelo classificador real).")
    p_hat = (df_test["sold"].to_numpy() > 0).astype(np.float32)

# 4) Carrega calibrações e modelos por (classe × h) e prevê qty_cond calibrado
calib = {}
calib_path = os.path.join(PRED_DIR, "calib_cls_h.joblib")
if os.path.exists(calib_path):
    calib = joblib.load(calib_path)

KEY = [STORE_COL, SKU_COL, "week", "h"]
pred_parts = []

for cls in ["smooth","intermittent","erratic","lumpy"]:
    cls_col = f"cls_{cls}"
    for h in [1, 2, 3, 4]:
        # fatia do df_test por one-hot + horizonte
        df_grp = df_test.filter((pl.col(cls_col) == 1) & (pl.col("h") == h))
        if df_grp.height == 0:
            continue

        mpath = os.path.join(PRED_DIR, f"reg_log_{cls}_h{h}.joblib")
        if not os.path.exists(mpath):
            # sem modelo do grupo -> qty_cond_hat = 0
            pred_parts.append(
                df_grp.select(KEY + [QTY_COL]).with_columns([
                    pl.lit(0.0, dtype=pl.Float32).alias("qty_cond_hat")
                ])
            )
            continue

        m = joblib.load(mpath)
        Xh = df_grp.select(feats_required).to_pandas().astype(np.float32)
        yhat_qty = np.expm1(m.predict(Xh, num_iteration=m.best_iteration_)).astype(np.float32)
        yhat_qty = np.clip(yhat_qty, 0.0, None)
        a = float(calib.get((cls, h), 1.0))
        qty_cond_hat = (a * yhat_qty).astype(np.float32)

        pred_parts.append(
            df_grp.select(KEY + [QTY_COL]).with_columns([
                pl.Series("qty_cond_hat", qty_cond_hat)
            ])
        )

# consolida qty_cond_hat por chave (se algum grupo faltou, preenche 0)
if pred_parts:
    df_qty = pl.concat(pred_parts, how="vertical")
else:
    # nenhum grupo produziu; cria qty_cond_hat=0 para todos
    df_qty = df_test.select(KEY + [QTY_COL]).with_columns([
        pl.lit(0.0, dtype=pl.Float32).alias("qty_cond_hat")
    ])

# junta de volta na mesma ordem de df_test
df_pred = (
    df_test.select(KEY + [QTY_COL] + feats_required)
           .join(df_qty.select(KEY + ["qty_cond_hat"]), on=KEY, how="left")
           .with_columns(pl.col("qty_cond_hat").fill_null(0.0).cast(pl.Float32))
)

# 5) Hurdle: p_hat × qty_cond_hat
p = np.clip(p_hat, 0.0, 1.0).astype(np.float32)
qty_cond_np = df_pred.get_column("qty_cond_hat").to_numpy().astype(np.float32)
yhat = (p * qty_cond_np).astype(np.float32)

y = df_pred.get_column(QTY_COL).to_numpy().astype(np.float32)


ae  = np.abs(y - yhat)
WMAPE = float(ae.sum()) / max(float(y.sum()), 1.0)
SSE   = float(((y - yhat) ** 2).sum())
SST   = float(((y - float(y.mean())) ** 2).sum()) + 1e-12
R2    = 1.0 - SSE / SST

print(f"WMAPE last4 = {WMAPE:.4f} | R² = {R2:.4f}")

# 7) Salva predições
outp = os.path.join(PRED_DIR, "preds_hurdle_v1_plus.parquet")
pl.DataFrame({
    STORE_COL: df_pred[STORE_COL],
    SKU_COL:   df_pred[SKU_COL],
    "week":    df_pred["week"],
    QTY_COL:   df_pred[QTY_COL],
    "p_hat":   p,
    "qty_cond_hat": df_pred["qty_cond_hat"],
    "pred_qty": yhat,
    "h":       df_pred["h"],
}).write_parquet(outp)
print("[OK] escrito:", outp)

# === A) FEATURE BUILDER SAFE (sem vazamento) ===
import polars as pl, math

def build_features_safe(lf_week, lf_ser, weeks_window, store_col, sku_col,
                        use_future_prices=False, us_per_week=7*24*3600*1_000_000):
    """
    weeks_window: lista de semanas necessárias (inclui lags + validação/teste)
    use_future_prices=False -> avg_price_net / discount_share também com lag 1
    """
    weeks_lf = pl.DataFrame({"week": weeks_window}).lazy()

    grid = (
        lf_week.select([store_col, sku_col]).unique()
               .join(weeks_lf, how="cross")
    )

    pnl = (
        grid.join(lf_week, on=[store_col, sku_col, "week"], how="left")
            .with_columns([
                pl.col("qty_wk").fill_null(0.0).cast(pl.Float32),
                pl.col("net_sum").fill_null(0.0).cast(pl.Float32),
                pl.col("gross_sum").fill_null(0.0).cast(pl.Float32),
            ])
            .sort([store_col, sku_col, "week"])
            .with_columns([
                (pl.col("qty_wk") > 0).cast(pl.Int8).alias("sold"),
                pl.when(pl.col("qty_wk") > 0).then(pl.col("qty_wk")).otherwise(0.0).alias("_qty_pos"),
            ])
            # ----- TODOS OS SINAIS PASSADOS: shift(1) -----
            .with_columns([
                pl.col("sold").shift(1).over([store_col, sku_col]).fill_null(0).cast(pl.Int8).alias("sold_prev"),
                pl.col("_qty_pos").shift(1).over([store_col, sku_col]).fill_null(0.0).alias("_qty_pos_prev"),
            ])
            .with_columns([
                pl.when(pl.col("sold_prev")==1).then(pl.col("week")).otherwise(None)
                  .forward_fill()
                  .over([store_col, sku_col])
                  .alias("_last_sale_week_prev"),
                pl.col("sold_prev").cum_sum().over([store_col, sku_col]).alias("_cum_sold_prev"),
                pl.col("_qty_pos_prev").cum_sum().over([store_col, sku_col]).alias("_cum_qty_prev"),
            ])
            .with_columns([
                ((pl.col("week").cast(pl.Int64) - pl.col("_last_sale_week_prev").cast(pl.Int64))
                   // pl.lit(us_per_week, dtype=pl.Int64))
                   .fill_null(99).cast(pl.Int16).alias("recency_weeks"),
                pl.col("_cum_sold_prev").cast(pl.Int16).alias("weeks_active_cum"),
                pl.when(pl.col("_cum_sold_prev") > 0)
                  .then(pl.col("_cum_qty_prev")/pl.col("_cum_sold_prev"))
                  .otherwise(0.0).cast(pl.Float32).alias("intensity_cum"),
            ])
    )

    # Preço/Desconto: se NÃO conhece futuro, use lag(1)
    if use_future_prices:
        pnl = pnl.with_columns([
            pl.when(pl.col("qty_wk") > 0).then(pl.col("net_sum")/pl.col("qty_wk")).otherwise(None).cast(pl.Float32).alias("avg_price_net"),
            pl.when(pl.col("gross_sum") > 0).then(1 - (pl.col("net_sum")/pl.col("gross_sum"))).otherwise(None).cast(pl.Float32).alias("discount_share"),
        ])
    else:
        pnl = pnl.with_columns([
            (pl.when(pl.col("qty_wk") > 0).then(pl.col("net_sum")/pl.col("qty_wk")).otherwise(None)
               .shift(1).over([store_col, sku_col]).cast(pl.Float32)).alias("avg_price_net"),
            (pl.when(pl.col("gross_sum") > 0).then(1 - (pl.col("net_sum")/pl.col("gross_sum"))).otherwise(None)
               .shift(1).over([store_col, sku_col]).cast(pl.Float32)).alias("discount_share"),
        ])

    pnl = pnl.with_columns([
        pl.col("week").dt.week().cast(pl.Int16).alias("weekofyear")
    ]).join(
        lf_ser.select([store_col, sku_col, "adi", "cv2", "zeros_pct", "demand_class"]),
        on=[store_col, sku_col], how="left"
    ).with_columns([
        pl.when(pl.col("demand_class")=="smooth").then(1).otherwise(0).cast(pl.Int8).alias("cls_smooth"),
        pl.when(pl.col("demand_class")=="intermittent").then(1).otherwise(0).cast(pl.Int8).alias("cls_intermittent"),
        pl.when(pl.col("demand_class")=="erratic").then(1).otherwise(0).cast(pl.Int8).alias("cls_erratic"),
        pl.when(pl.col("demand_class")=="lumpy").then(1).otherwise(0).cast(pl.Int8).alias("cls_lumpy"),
    ])

    # Lags (de quantidade)
    for k in (1,4,8):
        pnl = pnl.with_columns(
            pl.col("qty_wk").shift(k).over([store_col, sku_col]).fill_null(0.0).alias(f"lag_qty_{k}")
        )

    # Fourier (sem usar polars.sin/cos globais; use Expr)
    period = 53.0
    for k in (1,2,3):
        ang = (2.0*math.pi*k)*(pl.col("weekofyear").cast(pl.Float64)/period)
        pnl = pnl.with_columns([ang.sin().alias(f"woy_sin_{k}"), ang.cos().alias(f"woy_cos_{k}")])

    return pnl

# === B) SPLIT DE TEMPO ===
WK_PATH   = "/content/tmp_ml/wk.parquet"
SER_PATH  = "/content/tmp_ml/series_feats.parquet"
STORE_COL, SKU_COL, QTY_COL = "internal_store_id", "internal_product_id", "qty_wk"

lf_week = pl.scan_parquet(WK_PATH)
lf_ser  = pl.scan_parquet(SER_PATH)

weeks_all = (lf_week.select("week").unique()
             .filter(pl.col("week").dt.year()==2022)
             .sort("week").collect()["week"])
weeks_last4    = weeks_all[-4:]
weeks_val_prev4= weeks_all[-8:-4]
weeks_train_rest = weeks_all[:-8]

# === C) CLASSIFICADOR (p_hat) — fix de streaming e colunas ausentes ===
import os
import numpy as np
import polars as pl
import lightgbm as lgb
import joblib

FEATS_CLS = [
    "recency_weeks","weeks_active_cum","intensity_cum",
    "avg_price_net","discount_share","weekofyear",
    "adi","cv2","zeros_pct",
    "cls_smooth","cls_intermittent","cls_erratic","cls_lumpy",
    "lag_qty_1","lag_qty_4","lag_qty_8",
    "woy_sin_1","woy_sin_2","woy_sin_3","woy_cos_1","woy_cos_2","woy_cos_3",
]

# 1) Painel para treino/val (apenas janelas necessárias)
need_weeks_cls = list(weeks_train_rest) + list(weeks_val_prev4)
pnl_cls = build_features_safe(
    lf_week, lf_ser, need_weeks_cls,
    STORE_COL, SKU_COL,
    use_future_prices=False  # evita leak por preço da própria semana-alvo
)

# 2) Checa colunas realmente presentes (sem coletar dados)
avail_cols = set(pnl_cls.collect_schema().names())
feats_eff = [c for c in FEATS_CLS if c in avail_cols]

# 3) Coleta SEM streaming (streaming + windows/joins = instável aqui)
df_tr_pl = (
    pnl_cls
    .filter(pl.col("week").is_in(weeks_train_rest))
    .select([QTY_COL] + feats_eff)
    .collect()  # <- sem streaming
    .with_columns([pl.col(c).cast(pl.Float32).fill_null(0.0).alias(c) for c in feats_eff])
)
df_va_pl = (
    pnl_cls
    .filter(pl.col("week").is_in(weeks_val_prev4))
    .select([QTY_COL] + feats_eff)
    .collect()  # <- sem streaming
    .with_columns([pl.col(c).cast(pl.Float32).fill_null(0.0).alias(c) for c in feats_eff])
)

# 4) Polars -> pandas
df_tr_pd = df_tr_pl.to_pandas()
df_va_pd = df_va_pl.to_pandas()

Xtr = df_tr_pd[feats_eff].astype(np.float32)
ytr = (df_tr_pd[QTY_COL].values > 0).astype(np.int8)
wtr = np.clip(df_tr_pd[QTY_COL].values, 1.0, None).astype(np.float32)

Xva = df_va_pd[feats_eff].astype(np.float32)
yva = (df_va_pd[QTY_COL].values > 0).astype(np.int8)
wva = np.clip(df_va_pd[QTY_COL].values, 1.0, None).astype(np.float32)

clf = lgb.LGBMClassifier(
    objective="binary",
    learning_rate=0.05, n_estimators=2000,
    num_leaves=63, min_child_samples=80,
    subsample=0.8, colsample_bytree=0.8,
    reg_lambda=1.0, random_state=42, n_jobs=-1,
    # force_col_wise=True,  # descomente se RAM apertar
)
clf.fit(
    Xtr, ytr,
    sample_weight=wtr,
    eval_set=[(Xva, yva)],
    eval_sample_weight=[wva],
    eval_metric="binary_logloss",
    callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],
)
joblib.dump(clf, os.path.join("/content/tmp_ml/preds", "clf.joblib"))
print("[OK] classificador salvo. best_iter=", clf.best_iteration_)

# === D) REGRESSORES (log1p) + CALIBRAÇÃO em weeks_val_prev4 ===
import numpy as np, joblib, os, lightgbm as lgb

PRED_DIR = "/content/tmp_ml/preds"
os.makedirs(PRED_DIR, exist_ok=True)

# painel para regressão (treino + validação)
need_weeks_reg = list(weeks_train_rest) + list(weeks_val_prev4)
pnl_reg = build_features_safe(lf_week, lf_ser, need_weeks_reg, STORE_COL, SKU_COL, use_future_prices=False)

# h_map para a validação
weeks_sorted = sorted(weeks_val_prev4)
h_map = pl.DataFrame({"week": weeks_sorted, "h": [1,2,3,4]})

df_reg = pnl_reg.join(h_map, on="week", how="left")  # h só na validação

FEATS_REG = FEATS_CLS  # mesmo conjunto base

def fit_one_reg(df_tr, df_va):
    Xtr = df_tr.select(FEATS_REG).to_pandas().astype(np.float32)
    ytr = np.log1p(df_tr[QTY_COL].to_pandas().astype(np.float32))
    wtr = np.clip(df_tr[QTY_COL].to_numpy(), 1.0, None).astype(np.float32)

    Xva = df_va.select(FEATS_REG).to_pandas().astype(np.float32)
    yva = np.log1p(df_va[QTY_COL].to_pandas().astype(np.float32))
    wva = np.clip(df_va[QTY_COL].to_numpy(), 1.0, None).astype(np.float32)

    m = lgb.LGBMRegressor(
        objective="regression_l2", learning_rate=0.05, n_estimators=2000,
        num_leaves=63, min_child_samples=80, subsample=0.8, colsample_bytree=0.8,
        reg_lambda=1.0, random_state=42, n_jobs=-1
    )
    m.fit(Xtr, ytr, sample_weight=wtr,
          eval_set=[(Xva, yva)], eval_sample_weight=[wva], eval_metric="l2",
          callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)])
    return m

def slope_calib(y, yhat, lo=0.9, hi=1.3):
    num = float((yhat*y).sum()); den = float((yhat*yhat).sum()) + 1e-12
    return float(np.clip(num/den, lo, hi))

classes = ["smooth","intermittent","erratic","lumpy"]
models, calib = {}, {}

for cls in classes:
    # treino: toda a história exceto 8 semanas finais
    tr = df_reg.filter(pl.col("demand_class")==cls).filter(~pl.col("week").is_in(weeks_val_prev4))
    # 4 semanas de validação (tem coluna h)
    for h in [1,2,3,4]:
        va = df_reg.filter((pl.col("demand_class")==cls) & (pl.col("h")==h))
        if tr.height==0 or va.height==0:
            continue
        m = fit_one_reg(tr, va)
        models[(cls,h)] = m
        joblib.dump(m, os.path.join(PRED_DIR, f"reg_log_{cls}_h{h}.joblib"))

        # calibra slope em weeks_val_prev4
        Xva = va.select(FEATS_REG).to_pandas().astype(np.float32)
        yva_qty = va[QTY_COL].to_numpy().astype(np.float32)
        yhat_qty = np.expm1(m.predict(Xva, num_iteration=m.best_iteration_)).clip(0).astype(np.float32)
        a = slope_calib(yva_qty, yhat_qty)
        calib[(cls,h)] = a
        print(f"[OK] {cls} h{h}  best_iter={m.best_iteration_}  a={a:.3f}")

joblib.dump(calib, os.path.join(PRED_DIR, "calib_cls_h.joblib"))
print("Total modelos:", len(models))

# === E) INFERÊNCIA last4 (clean) ===
import numpy as np, polars as pl, os, joblib

clf = joblib.load(os.path.join(PRED_DIR, "clf.joblib"))
calib = joblib.load(os.path.join(PRED_DIR, "calib_cls_h.joblib"))

max_lag = 8
need_len = max_lag + 4 + 4
weeks_needed = list(weeks_all[-need_len:])

pnl_test = build_features_safe(lf_week, lf_ser, weeks_needed, STORE_COL, SKU_COL, use_future_prices=False)

# mapeia h=1..4 nas últimas 4
h_map_last4 = pl.DataFrame({"week": list(weeks_last4), "h": [1,2,3,4]})
df_test = (pnl_test.filter(pl.col("week").is_in(weeks_last4))
                  .join(h_map_last4, on="week", how="left"))

# p_hat (classificador real)
Xc = df_test.select(FEATS_CLS).to_pandas().astype(np.float32)
p_hat = clf.predict_proba(Xc)[:,1].astype(np.float32)

# qty_cond por classe×h com ajuste 'a'
pred_qty = np.zeros(df_test.height, dtype=np.float32)
for cls in ["smooth","intermittent","erratic","lumpy"]:
    for h in [1,2,3,4]:
        mask = ((df_test["demand_class"]==cls) & (df_test["h"]==h)).to_numpy()
        if not mask.any():
            continue
        mpath = os.path.join(PRED_DIR, f"reg_log_{cls}_h{h}.joblib")
        if not os.path.exists(mpath):
            continue
        m = joblib.load(mpath)
        Xh = df_test.filter(mask).select(FEATS_CLS).to_pandas().astype(np.float32)
        yhat_qty = np.expm1(m.predict(Xh, num_iteration=m.best_iteration_)).clip(0).astype(np.float32)
        a = calib.get((cls,h), 1.0)
        pred_qty[mask] = (a * yhat_qty).astype(np.float32)

# Hurdle
p = np.clip(p_hat, 0.0, 1.0).astype(np.float32)
yhat = (p * pred_qty).astype(np.float32)

y = df_test.get_column(QTY_COL).to_numpy().astype(np.float32)
ae = np.abs(y - yhat)
WMAPE = float(ae.sum()) / max(float(y.sum()), 1.0)
SSE   = float(((y - yhat)**2).sum())
SST   = float(((y - float(y.mean())) ** 2).sum()) + 1e-12
R2    = 1.0 - SSE / SST

print(f"WMAPE last4 = {WMAPE:.4f} | R² = {R2:.4f}")

outp = os.path.join(PRED_DIR, "preds_hurdle_v1_clean.parquet")
pl.DataFrame({
    STORE_COL: df_test[STORE_COL],
    SKU_COL:   df_test[SKU_COL],
    "week":    df_test["week"],
    QTY_COL:   df_test[QTY_COL],
    "p_hat":   p,
    "qty_cond_hat": pred_qty,
    "pred_qty": yhat,
}).write_parquet(outp)
print("[OK] escrito:", outp)